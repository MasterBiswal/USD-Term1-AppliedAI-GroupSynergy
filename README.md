# ğŸ§¬ Multi-Label Mutation Tag Prediction Using Machine Learning

This repository contains a complete pipeline for predicting multiple mutation tags from high-dimensional biological data. The project includes data preprocessing, exploratory data analysis (EDA), dimensionality reduction, and multi-label classification using models like Logistic Regression, Random Forest, LightGBM, and XGBoost.

---

## ğŸ“‚ Project Structure

```
.
ğŸ”˜ data/
â”‚   ğŸ”˜ raw/                 # Contains original K9.data and K9.instance.tags
â”‚   â””ğŸ”˜ processed/           # Cleaned CSVs, encoded labels, and tag classes
ğŸ”˜ artifacts/               # Dimensionality reduced features and labels
ğŸ”˜ saved_model/             # Trained XGBoost model (.pkl)
ğŸ”˜ notebooks/               # Jupyter/Colab notebooks for analysis and training
ğŸ”˜ README.md                # This file
ğŸ”˜ requirements.txt         # All dependencies
```

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/MasterBiswal/USD-Term1-AppliedAI-GroupSynergy
cd mutation-tag-prediction
```

### 2. Create and activate a virtual environment (optional but recommended)

```bash
python -m venv .venv
source .venv/bin/activate       # On Windows: .venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ“¦ Dependencies

These are the key Python libraries used in this project:

```text
pandas
numpy
joblib
matplotlib
seaborn
scikit-learn
xgboost
lightgbm
```

All are listed in `requirements.txt`. You can generate or update it using:

```bash
pip freeze > requirements.txt
```

---

## ğŸ“Š Project Workflow

### ğŸ”¹ 1. Data Cleaning

- Parse raw `.tags` and `.data` files
- Align tags and features
- Handle missing values and dimensional mismatches
- Encode multi-label tags with `MultiLabelBinarizer`

### ğŸ”¹ 2. Exploratory Data Analysis (EDA)

- Analyze feature sparsity and tag distributions
- Visualize tag frequency
- Correlation analysis and dimensionality reduction via Truncated SVD

### ğŸ”¹ 3. Modeling

- Baseline: Logistic Regression (OvR)
- Random Forest (OvR)
- Final: XGBoost (OvR) â€“ Best performance with F1-micro â‰ˆ 0.995

### ğŸ”¹ 4. Model Evaluation

- Classification reports
- Micro and macro F1 scores
- Hamming loss and subset accuracy

### ğŸ”¹ 5. Model Export

- Trained model saved as `xgb_multilabel_model.pkl` using `joblib`

---

## ğŸ“ˆ Performance Summary (XGBoost)

| Metric            | Score |
| ----------------- | ----- |
| Micro F1 Score    | 0.995 |
| Macro F1 Score    | 0.985 |
| Weighted F1 Score | 1.000 |
| Samples Avg. F1   | 0.990 |

---

## ğŸ§ª Reproducibility

Ensure your directory structure matches the expected format:

```text
project_root/
ğŸ”˜ data/
â”‚   ğŸ”˜ raw/ (place your original K9.data and K9.instance.tags here)
â”‚   â””ğŸ”˜ processed/
ğŸ”˜ artifacts/
ğŸ”˜ saved_model/
```

Run all scripts or notebooks in order, or load up the Jupyter notebook version to experiment interactively.

---

## ğŸ§° Future Improvements

- Add SHAP or LIME interpretability
- Handle rare tags using meta-labeling or oversampling (e.g., SMOTE)
- Build REST API or Streamlit interface
- Evaluate on external datasets

---

## ğŸ“„ License

This project is under the MIT License. See the LICENSE file for details.

---

## ğŸ™Œ Acknowledgments

- scikit-learn, LightGBM, and XGBoost teams for powerful ML libraries
- INRIA and OpenML for benchmarking techniques
- Your team, collaborators, and professors for feedback and guidance

---

## ğŸ¤ Contributions

Feel free to fork, raise issues, or submit pull requests to improve this project!

